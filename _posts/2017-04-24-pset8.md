---
layout: post
title: "Problem Set 8 (alpha version)"
date:   Mon Apr 24 11:16:20 EDT 2017
author: Alex Beatson and Sebastian Seung
---
<style>
.center-image
{
    margin: 0 auto;
    display: block;
}
</style>

$$
\DeclareMathOperator*\trace{Tr}
\DeclareMathOperator*\argmax{argmax}
\DeclareMathOperator*\argmin{argmin}
$$

### Due at 3pm on Monday, May 1. Please submit to Blackboard and follow the general [guidelines](https://cos495.github.io/general/2017/02/06/homework-guidelines.html) regarding homework assignments.

In this assignment, you will study the problem of predicting the next character of text.  You'll start with unigram and bigram models, and progress to recurrent neural networks.

### Programming exercises

Use `git pull` to update your cos495 sample code repo.

1. Unigram model.  The notebook `Unigram.ipynb` reads in the text corpus `shakespeare.txt` using [text-handling functions](https://en.wikibooks.org/wiki/Introducing_Julia/Working_with_text_files), uses the `unique` function to get a list of the characters in the corpus, and counts the number of times each character appears in the corpus using [string functions](https://en.wikibooks.org/wiki/Introducing_Julia/Strings_and_characters). Run the code and you will see a bar graph of the counts.  The characters are ordered so that the counts decrease from left to right. 

   - What are the three most frequent characters? What are the three least frequent characters?
   
   - Divide the corpus into training and test sets, reserving the last 10% of the corpus for the test set.  Repeat the counts calculation above for the training set only.  Divide the counts by the length of the training set to estimate the probability distribution of the [unigram model](https://en.wikipedia.org/wiki/Language_model#Unigram_models).  What is the [perplexity](https://en.wikipedia.org/wiki/Perplexity) per letter of your unigram model for the training and test sets?  What is the cross entropy per letter (in bits) for the training and test sets?  Submit your code as well as numerical answers.
   
   - Generate 1000 characters from your unigram model by sampling from the unigram probability distribution.  Hint: If `p` is an $$n$$-dimensional vector, then `rand(Categorical(p))` will generate a random integer `i` from 1 to $$n$$ with probability `p[i]`.  The function `Categorical` comes from the `Distributions.jl` package and implements a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution).
   
2. Bigram model. Write code to estimate the conditional probability distributions of the [bigram model](https://en.wikipedia.org/wiki/Language_model#n-gram_models).

   - What are the three most frequent bigrams?  What are the three least frequent bigrams (excluding those that don't occur at all)?

   - What is the perplexity per letter of your bigram model?  What is the cross entropy per letter (in bits)?  Submit numerical answers for both training and test sets, as well as your code.

   - Generate 1000 characters from your bigram model.

3. [Elman network](https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks). Here you will use a recurrent net to predict the next character in the Shakespeare corpus.  It won't produce samples indistinguishable from Shakespeare, but some structure should emerge from training.

   - Look at `Elman.ipynb`, and write the equations for the recurrent network implemented in the code, using letters that correspond to the variable names in the code. What is the input vector?  What is the output vector?

   - Modify the code to train only on the training set. Run the code for 500,000 steps. Now calculate the cross entropy per letter in bits on the test set.  Is this better or worse than your unigram and bigram models?

   - Experiment with sampling from the model with different temperature parameters. Print one sample with temperature very close to 0 and one with an extremely high temperature. What does temperature qualitatively do to the samples?

2. LSTM network. 

   - Modify the code to implement the equations of the [traditional LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory#Traditional_LSTM).
   
   - Hint: as well as changing the "step" and "init_params" functions to include the LSTM equations and parameters, you need to ensure the other methods can handle the cell state as well as the hidden state. One way to do this is to add a *c* everywhere there is an *h*. Another way to do this is to change the size of *h* to 2x *hidden_size* everywhere outside of "step", and within "step" (i) use tf.split to split the input *h* (size 2x *hidden_size*) into *hidden* and *cell* (both size 1x *hidden_size*), (ii) compute the new *hidden* and *cell*, and (iii) use tf.concat to merge them into a new *h* to return from "step".

   - Once training is done, calculate the cross entropy per letter in bits on the test set.  Is this better or worse than the Elman net?
