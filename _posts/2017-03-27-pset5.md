---
layout: post
title: "Problem Set 5 (alpha version)"
date:  Mon Mar 27 10:13:18 EDT 2017
author: Sebastian Seung
---
\\(
\DeclareMathOperator*\argmax{argmax}
\\)

### Due at 3pm on Monday, Apr. 3. Please submit to Blackboard and follow the general [guidelines](https://cos495.github.io/general/2017/02/06/homework-guidelines.html) regarding homework assignments.

This assignment is based on the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  This consists of 32x32 RGB images in 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.  The images are rather low resolution, but still generally recognizable.

We assume that you have set up a Jupyter notebook that is communicating with an EC2 instance, following our AWS [instructions](https://cos495.github.io/general/2017/03/27/AWS.html).  (Or you can do the whole thing locally; no instructions provided but you can find them online elsewhere.)  If you are using Jupyter to communicate with an EC2 instance, select `New > Terminal` if you want to interact with a shell.  (You can also ssh to the instance using your private key, as explained in the other blog post.)  Type `git clone https://github.com/cos495/code.git`.  From the Jupyter dashboard, navigate to `code` and then open the `CIFAR10` or `CIFAR10_Python` notebook. 

Google supports a Python API for use of TensorFlow.  The Julia API is similar, and is a community effort [documented here](https://malmaud.github.io/tfdocs/index.html).

1. Understanding the architecture

   - How many kernels are in the first convolutional layer?
   
   - How many kernels are in the second convolutional layer?
   
   - How many kernels are in the third convolutional layer?
   
   - After the third convolutional layer, how many feature maps are there?  How large is each feature map?
   
   - How many fully connected layers are there, and how large are the weight matrices?

2. Visualization. You can convert the first layer kernels into a Julia array using `run(session, weights["wc1"])` or into a Numpy array using `sess.run(weights['wc1'])`.  Display the kernels as RGB images.

3. Data augmentation.  If you run for at least 2000 minibatches, you should see evidence of overfitting in the loss on the validation set.  Write code that left-right reflects a random subset of images in the minibatch.  Is overfitting reduced?  Does the validation error improve?
   
4. Architecture modifications.  Submit code and learning curves for two modified architectures of your choice.  For example, you could vary the number of 

   - feature maps in a convolutional layer
   
   - convolutional layers
   
   - maxpooling layers
   
   - fully connected layers

5. Open competition (extra credit). Can you beat the [state-of-the-art](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html) or at least your classmates?  Try any trick you've learned from class or papers, or invent your own.  For example, you could try other architectures, dropout, or other data augmentation techniques such as random translations (horizontal and vertical) and color jitter. You're honor-bound to not train or do model selection based on the test set.  Once you've selected your model based on the validation set, quantify its error on the test set and then submit.  The three submissions with the best test error will receive modest prizes.  
